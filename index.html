<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="google-site-verification" content="H7ktul5IaEVw3s4yG0jNsCEDEhqCjp4UZVHvTD-hNiE" />
  <meta name="baidu-site-verification" content="codeva-n2QUSLQsRx" />
  <title>Rongwu Xu (ËÆ∏ËûçÊ≠¶)</title>
  <link rel="stylesheet" href="stylesheets/styles.css">
  <link rel="stylesheet" href="stylesheets/pygment_trac.css">
  <meta name="viewport" content="width=device-width">
  <script src="./javascripts/load_navbar.js"></script>
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
</head>

<body>
  <!-- <nav class="navbar">
    <div class="nav-content">
      <a href="./index.html">Home</a>
      <a href="./bio.html">Bio</a>
      <a href="./publications.html">Publications</a>
      <a href="./cv.html">CV</a>
      <a href="./misc.html">Misc</a>
    </div> -->
  </nav>

  <div class="wrapper">
    <header>
      <h1>Rongwu Xu (ËÆ∏ËûçÊ≠¶)</h1>
      <img src="./index_files/profile.jpg" alt="me" class="profile-image">
      <p style="color:#777;font-size: 95%;">0xrwxu@gmail.com<em> or</em><br>xrw22@mails.tsinghua.edu.cn</p>
      <p>
        <a href="https://github.com/pillowsofwind/">GitHub
        </a> | <a href="https://x.com/rongwu_xu">X (Twitter)</a> | <a
          href="https://www.linkedin.com/in/rongwu-xu-5322b532a/">LinkedIn</a>
        <br>
        <a href="https://scholar.google.com/citations?user=HyjNrDMAAAAJ">Google Scholar</a> | <a
          href="https://openreview.net/profile?id=~Rongwu_Xu1">OpenReview</a>
      </p>
    </header>

    <section>
      <h2>Research</h2>
      <p>
        I am an artificial intelligence (AI) researcher with pretty interdisciplinary interests. I try to understand how
        AI's design and its
        interactions with humans
        can lead to unexpected behaviors and increased societal risks. I approach this work through the lens of
        behavioral experiments, machine learning (ML), interpretability tools, and psychology. I publish my
        findings in the Natural Language Processing (NLP), AI, and ML communities.
      </p>

      <p>
        Currently, I am interested in the following research topics:
      </p>
      <ol>
        <li><b>AI Safety and Alignment</b>: Identifying potential safety and ethics risks associated with AI R&D and
          developing
          strategies to align AI systems with human values, behaviors, and expectations.</li>
        <li><b>Machine Behavior</b>: Investigating the similarities and differences between AI models and human
          behaviors, and utilizling psychology-inspired experiments to test and understand machines.</li>
        <li><b>AI and Psychology</b>: Studying both the understanding the psychological impacts of AI systems on humans
          (Psychology of AI, a subset of Psychology of Technology) and the application of AI in psychological research
          (AI for
          Psychology, a subset of AI for Science).</li>
      </ol>
      <p>
        My other general interests include model evaluation and real-world applications of such models.
      </p>

      <h2>News</h2>
      <p>AI/NLP research can be challenging for newcomers. If you're interested in my work or have ideas to explore, I'd
        be happy to guide you. We can work on submitting papers to top venues.
        <strong>Feel free to drop me an <a href="mailto:0xrwxu@gmail.com">Email</a> if interested.</strong>
      </p>

      <div class="slidery">
        <ul>
          <li><b>May 2025</b> Checkout our new review paper on AI awareness! <a
              href="https://arxiv.org/pdf/2504.20084">[Paper]</a><a href="https://ai-awareness.github.io/">[Project
              Page]</a></li>
          <li>
            <b>Apr 2025</b> I am attending two AI safety & alignment conferences co-located with ICLR
            2025 (Singapore): The Misalignment and Control Workshop (Apr 24th, our new paper on catastrophic risks
            and deception of LLM agents will be presented <a href="https://arxiv.org/pdf/2502.11355">[Paper]</a><a
              href="https://llm-catastrophic-risks.github.io/">[Project Page]</a>) and The Singapore Conference on AI
            (SCAI) (Apr 26th).
          </li>
          <li>
            <b>Mar 2025</b> Got accepted to UIUC CS, UW CSE and JHU CS. Grateful to the opportunities!
          </li>
          <li>
            <b>Jan 2025</b> I am looking for PhD opportunities starting 2025. Don't hesitate to reach out if you think I
            can be a good candidate.
          </li>
          <li>
            <b>Oct 2024</b> Six papers accepted to EMNLP 2024! Thanks to my collaborators!
          </li>
          <li>
            <b>Sep 2024</b> I received the National Scholarship by the Ministry of Education of China!
          </li>
          <li>
            <b>Aug 2024</b> My paper "The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation
            via
            Persuasive Conversation" recieved an Outstanding Paper Award at ACL 2024!
          </li>
          <li>
            <b>Jul 2024</b> Check out <a
              href="https://www.bilibili.com/video/BV1Bi421h7Rn/?buvid=Z5422F58DED256C14DD4BC3CB72F80B9EF07">our talk
              (Chinese)</a> on knowledge conflicts for (RAG) LLMs! <a
              href="https://arxiv.org/pdf/2403.08319">[Paper]</a><a
              href="https://github.com/pillowsofwind/Knowledge-Conflicts-Survey">[Resource]</a><a
              href="https://mp.weixin.qq.com/s/y9-DwgNb3Yftgf_Ulf6yDQ">[Êú∫Âô®‰πãÂøÉ]</a><a
              href="./index_files/slides/Knowledge conflict.pdf">[Slides]</a>
          </li>
          <li>
            <b>May 2024</b> Two papers accepted to ACL 2024! Thanks to my collaborators!
          </li>
          <li>
            <b>May 2024</b> Check out LLMs' safety vulnerabilities discovered by tricking them to believe in
            misinformation! <a href="https://arxiv.org/pdf/2312.09085">[Paper]</a><a
              href="https://llms-believe-the-earth-is-flat.github.io">[Resource]</a><a
              href="https://mp.weixin.qq.com/s/zTqo4PvIaktg7fTCN5Mmdg">[Êú∫Âô®‰πãÂøÉ]</a><a
              href="https://mp.weixin.qq.com/s/fyer-SJeIBLkm0-Umt2Bcw">[Video]</a>
          </li>
          <li>
            <b>Apr 2024</b> I passed the PhD qualification exam (preliminary+oral) at IIIS, Tsinghua!
          </li>
          <li>
            <b>Dec 2023</b> I recieved the overall execellence scholarship at Tsinghua!
          </li>
          <li>
            <b>Apr 2023</b> One paper accepted to EuroS&P 2023! Thanks to my collaborators!
          </li>
          <li>
            <b>Dec 2022</b> Debut of my academic homepage.
          </li>
          <li>
            <b>Aug 2022</b> Enrolled as a graduate student at IIIS, Tsinghua University.
          </li>
        </ul>
      </div>

      <h2>Selected Publications</h2>


      <ul>
        <li>
          Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents<br>
          <b>Rongwu Xu*</b>, Xiaojian Li*, Shuo Chen*, Wei Xu<br>
          arXiv Preprint<br>
          <a href="https://arxiv.org/pdf/2502.11355">[Paper]</a><a
            href="https://llm-catastrophic-risks.github.io/">[Project Page]</a><a
            href="https://github.com/pillowsofwind/LLM-CBRN-Risks">[Code]</a><a
            href="https://x.com/rongwu_xu/status/1894049089525862745">[X Post]</a><a
            href="./index_files/slides/Alignment Workshop PPT_full.pdf">[Slides]</a<br><a
              href="https://aisafetychina.substack.com/p/ai-safety-in-china-19">[AI Safety China]</a>
        </li>

        <li>
          Knowledge Conflicts for LLMs: A Survey<br>
          <b>Rongwu Xu*</b>, Zehan Qi*, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, Wei Xu<br>
          EMNLP 2024 <br>
          <a href="https://arxiv.org/pdf/2403.08319">[Paper]</a><a
            href="https://github.com/pillowsofwind/Knowledge-Conflicts-Survey">[Code]</a><a
            href="https://mp.weixin.qq.com/s/y9-DwgNb3Yftgf_Ulf6yDQ">[Êú∫Âô®‰πãÂøÉ]</a><a
            href="https://www.bilibili.com/video/BV1Bi421h7Rn/?buvid=Z5422F58DED256C14DD4BC3CB72F80B9EF07">[Talk
            (Chinese)]</a><a href="./index_files/slides/Knowledge conflict.pdf">[Slides]</a><a
            href="./index_files/poster/EMNLP24-Poster-MAIN-KC.pdf">[Poster]</a><a
            href="https://x.com/rongwu_xu/status/1805452751268192330">[X Post]</a><br>
        </li>



        <li>
          How Alignment and Jailbreak Work: Explain LLM Safety through
          Intermediate Hidden States<br>
          Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, <b>Rongwu Xu</b>, Fei Huang, Yongbin Li<br>
          EMNLP 2024 Findings <br>
          <a href="https://arxiv.org/pdf/2406.05644">[Paper]</a><a
            href="https://github.com/ydyjya/LLM-IHS-Explanation">[Code]</a><a
            href="./index_files/poster/EMNLP24-Poster-FIND.pdf">[Poster]</a><br>
        </li>



        <li>
          The Earth is Flat because...: Investigating LLMs' Belief towards
          Misinformation via Persuasive Conversation<br>
          <b>Rongwu Xu</b>, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei
          Xu,
          Han Qiu<br>
          ACL 2024 <span style="background-color: rgb(255, 255, 100)">Oral</span> <br>
          <strong style="color: rgb(255, 0, 0);">üèÜ Outstanding Paper Award</strong> <a
            href="./index_files/awards/acl2024.jpg">[Certificate]</a><br>
          <a href="https://arxiv.org/pdf/2312.09085">[Paper]</a><a
            href="https://llms-believe-the-earth-is-flat.github.io">[Project Page]</a><a
            href="https://github.com/LLMs-believe-the-earth-is-flat/llms-believe-the-earth-is-flat">[Code]</a><a
            href="https://mp.weixin.qq.com/s/zTqo4PvIaktg7fTCN5Mmdg">[Êú∫Âô®‰πãÂøÉ]</a><a
            href="https://iiis.tsinghua.edu.cn/show-10561-1.html">[Video]</a><a
            href="./index_files/poster/ACL24-Poster-2629.pdf">[Poster]</a><a
            href="./index_files/slides/the_earth_is_flat.pdf">[Slides]</a><br>
        </li>


      </ul>

      <p>(* equal contribution, ‚Ä† corresponding author)</p>


      <h2>Awards</h2>

      <ul>
        <li>Most Recognized Research Outcomes at Tsinghua University Nomination (Ê∏ÖÂçéÂ§ßÂ≠¶ÊúÄÂèóÂ∏àÁîüÂÖ≥Ê≥®ÁöÑÂπ¥Â∫¶‰∫ÆÁÇπÊàêÊûúÊèêÂêç, Top 2 at
          IIIS), 2024</li>
        <li>
          Tsinghua University Excellent Teaching Assistant (Ê∏ÖÂçéÂ§ßÂ≠¶‰ºòÁßÄÂä©Êïô, Top 2%), 2024 </li>
        <li>
          National Scholarship (ÂõΩÂÆ∂Â•ñÂ≠¶Èáë, Top 1%), 2024
        </li>
        <li>
          Tsinghua University Outstanding Student Cadre (Ê∏ÖÂçéÂ§ßÂ≠¶‰ºòÁßÄÂ≠¶ÁîüÂπ≤ÈÉ®, Top 1.5%, 121 out of 9000+), 2024 </li>
        <li>
          ACL 2024 Outstanding Paper Award (Top 0.79%, 35 out of 4407), 2024
        </li>
        <li>
          Tsinghua-Yangtze River Delta International R&D Community Talent
          Scholarship, 2023
        </li>
        <li>
          Tsinghua University Overall Excellence Scholarship (Top 10%), 2023
        </li>
        <li>
          Tsinghua University Overall Excellence Scholarship (Top 10%), 2022
        </li>
        <li>
          Tsinghua University Technological Innovation Excellence Scholarship, 2020
        </li>
        <li>
          Tsinghua-Panasonic Scholarship (Top 10%), 2019
        </li>
        <li>
          Outstanding Volunteers in Beijing, 2018
        </li>
      </ul>



      <h2>Talks</h2>

      <ul>
        <li>
          <em>Catastrophic Risks and Deception of LLM Agents</em> <a
            href="./index_files/slides/Alignment Workshop PPT_full.pdf">[Slides]</a><br>
          - Misalignment and Control Workshop (w. Concordia AI, FAR.AI, etc), Singapore, Apr 2025
        </li>

        <li>
          <em>The Choice of Research</em> (‚ÄúÂÅöÁßëÁ†îÁöÑÈÄâÊã©‚Äù)<br>
          - <a href="https://rongwuxu.com/index_files/speech/2024asStudentRepresentatives.pdf">Speech as a student
            representative at the 2024 IIIS opening ceremony</a> (Âú®2024Âπ¥Èô¢ÂºÄÂ≠¶ÂÖ∏Á§º‰∏ä‰Ωú‰∏∫Âú®Ê†°Áîü‰ª£Ë°®ÁöÑÂèëË®Ä), IIIS, Tsinghua, Sep 2024
        </li>

        <li>
          <em>Investigating LLMs' Beliefs and Behaviors Under Persuasive Misinformation</em> <a
            href="./index_files/slides/the_earth_is_flat.pdf">[Slides]</a><br>
          - Oral report@ACL conference, Bangkok, Thailand, Aug 2024<br>
          - <a href="https://mp.weixin.qq.com/s/fyer-SJeIBLkm0-Umt2Bcw">Propaganda film</a>, IIIS, Tsinghua, Apr
          2024
        </li>

        <li>
          <em>Knowledge Conflicts for (RAG) LLMs</em> <a
            href="./index_files/slides/Knowledge conflict.pdf">[Slides]</a><br>
          - <a href="https://www.bilibili.com/video/BV1Bi421h7Rn/?vd_source=a6a7351f290ba7028b4cbe8932f50caa">Online
            talk</a>, NICE (w. Soochow University), Jul 2024<br>
          <iframe
            src="https://player.bilibili.com/player.html?isOutside=true&aid=1456305681&bvid=BV1Bi421h7Rn&cid=1621518948&p=1&autoplay=0&muted=0"
            scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true">
          </iframe>
        </li>

        <li>
          <em>Privacy-preserving Authentication using TEE</em> <a href="./index_files/slides/MISO.pdf">[Slides]</a><br>
          - Oral report, EuroS&P conference, Delft, The Netherlands, May 2023
        </li>
      </ul>

      <h2>Professional Service</h2>

      <ul>
        <li>Peer Reviewer, ACL Rolling Review, 2024-Present <br>
          <em>Tracks:</em> Ethics, Bias, and Fairness, Human-Centered NLP (2025-), NLP Applications, Resources and
          Evaluation
        </li>
        <li>Peer Reviewer, IEEE Access, 2025
        </li>
      </ul>


    </section>
    <footer>
      <p><img
          src="http://clustrmaps.com/map_v2.png?cl=383838&w=a&t=tt&d=4c3GJeTjb0tVZ0ms8bO0eS0t1QNKn-ijMkOTee58jBQ&ct=e0e0e0&co=005ea1" />
      </p>

      <p>¬© Copyright 2022-2025 Rongwu Xu.</p>
      <!-- 
      <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small>
      </p> -->
    </footer>
  </div>
  <script src="javascripts/scale.fix.js"></script>


</body>

</html>